import { Ollama } from "@llamaindex/ollama";
import { Settings } from "llamaindex";
import readline from "readline";

// Configura el modelo Ollama
// Aseg煤rate de tener el modelo descargado y corriendo en tu m谩quina
// Baja ollama  de https://ollama.com/ 
// corre el siguiente comando en la terminal: `ollama run gemma3:1b`
const ollamaLLM = new Ollama({
  model: "gemma3:4b", // Cambia el modelo si lo deseas por ejemplo : "mistral:7b", "llama2:7b", etc.
  temperature: 0.75,
});

// Asigna Ollama como LLM y modelo de embeddings
Settings.llm = ollamaLLM;
Settings.embedModel = ollamaLLM;

// Funci贸n principal del programa
async function main() {

  const rl = readline.createInterface({
    input: process.stdin,
    output: process.stdout,
  });

  let history = [
    {
      role: "system",
      content: "Eres un agente virtual cuyo prop贸sito es ayudar a la gente a definir su trayectoria educativa o laboral. Debes hacer al menos 3 preguntas abiertas sobre gustos, intereses y preferencias personales, para luego sugerir al menos 2 posibilidades de estudios en base a los datos del usuario. No hagas m谩s preguntas que las necesarias para hacer recomendaciones de carreras. Tienes que mantener un tono amable y claro en todo momento."
    },
  ]

  console.log(" Bot con IA (Ollama) iniciado.");
  console.log("Escrib铆 tu pregunta o pon茅 'salir' para terminar:");

  // Escuchamos cada vez que el usuario escribe algo
  rl.on("line", async (input) => {
    if (input.toLowerCase() === "salir") {
      rl.close(); // Cerramos el programa si escribi贸 "salir"
      return;
    }

    // Guardamos la pregunta del usuario
    history.push({ role: "user", content: input });

    try {
      // Enviamos la conversaci贸n al modelo de IA usando Ollama
      const res = await ollamaLLM.chat({ messages: history });

      // Guardamos la respuesta del LLM
      history.push(res.message);

      // Obtenemos el texto de la respuesta
      const respuesta = res?.message?.content || res?.message || "";

      // Mostramos la respuesta en consola
      console.log(" IA:", respuesta.trim());
    } catch (err) {
      // Si hay un error lo mostramos
      console.error("锔 Error al llamar al modelo:", err);
    }

    console.log("\nPregunt谩 otra cosa o escrib铆 'salir':");
  });
}

// Ejecutamos la funci贸n principal
main();
